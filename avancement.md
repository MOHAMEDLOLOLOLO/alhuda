#Ã©tat d'avancement :Obstacle Detection AI

## ğŸ¯ Objectif
DÃ©velopper un prototype de **lunettes intelligentes** capables de dÃ©tecter les obstacles lors de la marche,
grÃ¢ce Ã  une camÃ©ra embarquÃ©e et un traitement IA dÃ©portÃ© sur PC.

## ğŸ§© Architecture actuelle
- **CamÃ©ra** : module camÃ©ra du Raspberry Pi Zero 2 W.
- **Transmission** : flux vidÃ©o H.264 envoyÃ© via UDP (`udpsink`) au PC.
- **Traitement IA** : sur PC, en Python avec PyTorch / OpenCV.
- **Affichage** : annotation en direct (`Obstacle` / `Pas obstacle`).
- **Communication retour** (prÃ©vu) : envoi dâ€™alerte UDP vers le Pi pour retour haptique ou sonore.

---

## âš™ï¸ Ã‰tapes dÃ©jÃ  rÃ©alisÃ©es

### ğŸ§  1. Prototype de classification binaire 1
- ModÃ¨le utilisÃ© : **MobileNetV3-Small (prÃ©-entraÃ®nÃ© ImageNet)**.
- AdaptÃ© Ã  **2 classes** : `Obstacle` / `Pas obstacle`.
- InfÃ©rence directe sur les frames du flux UDP (FFmpeg ou GStreamer).
- Pipeline vidÃ©o :
  ```bash
  Raspberry â†’ UDP â†’ PC â†’ MobileNetV3 â†’ Affichage + DÃ©tection

tester:
//editer la connection plus avec wpa.coonf mais avec sudo nmtui

Sur le Raspberry Pi â€” Ã‰mission du flux vidÃ©o

Lance la commande suivante :
rpicam-vid -t 0 \
  --width 640 --height 480 --framerate 30 \
  --codec h264 --inline --listen \
  -o udp://<IP_PC>:5000
  
Sur le PC â€” RÃ©ception et traitement IA

CrÃ©e un fichier obstacle_simple.py
lancer obstacle_simple.py

rÃ©sultat:
<img width="1451" height="1091" alt="Capture d'Ã©cran 2025-10-28 163614" src="https://github.com/user-attachments/assets/a9e77bf0-5f01-4b2a-97fd-ebc2c7a40e20" />
<img width="1368" height="1113" alt="Capture d'Ã©cran 2025-10-28 163652" src="https://github.com/user-attachments/assets/4393ffc7-d95a-4c0a-b961-0075d14f52a6" />

SUITE 

ğŸš€ 2. DÃ©tection dâ€™obstacles par localisation (YOLOv8 + SafeWalkBD)
ğŸ” Objectif de cette Ã©tape

AmÃ©liorer la prÃ©cision et la pertinence de la dÃ©tection en remplaÃ§ant la simple classification binaire (Obstacle / Pas obstacle) par une dÃ©tection dâ€™objets.
Lâ€™objectif est dâ€™identifier les obstacles visibles et de dÃ©terminer sâ€™ils se trouvent rÃ©ellement sur la trajectoire de marche (zone centrale de lâ€™image).

ğŸ§  ModÃ¨le utilisÃ©

ModÃ¨le : YOLOv8 (Ultralytics)

Dataset : SafeWalkBD (hÃ©bergÃ© sur Roboflow Universe
)

Poids : best.pt (version YOLOv8, tÃ©lÃ©chargÃ©e automatiquement depuis Roboflow)

Langage : Python 3.10+

Librairies principales :

ultralytics â†’ pour le modÃ¨le YOLOv8

opencv-python â†’ pour le flux vidÃ©o et lâ€™affichage

roboflow â†’ pour le tÃ©lÃ©chargement automatique du modÃ¨le

âš™ï¸ Fonctionnement actuel

Le PC reÃ§oit en temps rÃ©el le flux vidÃ©o du Raspberry Pi (H.264 / UDP) et applique le modÃ¨le YOLOv8.
Chaque objet dÃ©tectÃ© est analysÃ© pour vÃ©rifier sâ€™il se trouve dans une zone centrale basse de lâ€™image â€” correspondant Ã  la direction de marche de lâ€™utilisateur.

Si un objet est dÃ©tectÃ© dans cette zone :
â¡ï¸ OBSTACLE
Sinon :
â¡ï¸ LIBRE


code dans obstacle_yolov8_centre.py
